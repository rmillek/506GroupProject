---
title: '506 Project: Group 11'
author: "Alex Kellner, Reed Millek, Zhaobo Wu"
date: "November 27, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Linear Discriminant Analysis

### What is Linear Discriminant Analysis?

Linear Discriminant Analysis (LDA) is most commonly used as dimensionality reduction technique in the pre-processing step for pattern-classification and machine learning applications. The goal is to project a dataset onto a lower-dimensional space with good class-separability in order avoid overfitting.

### Dataset: Geometric Properties of Wheat Seeds

For our data analysis, we will be using the "Seeds" dataset, found on [UC Irvine's Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/seeds#). This dataset is comprised observations of kernels from 3 different varieties of wheat: Kama, Rosa and Canadian. Each group has randomly selected 70 observations, for a total of 210 observations. Aside from the grouping variable, there are 7 geometrical measurments given to each observation:

1. area = Area
2. peri - Perimeter
3. comp = Compactness (C = 4\*pi\*A/P^2)
4. l = Length of kernel
5. w = Width of kernel
6. Asym = Asymmetry coefficient
7. lgroove = Length of kernel groove

We will be using each qualitative attribute to classify each group, and once a fit model has been obtained, make predictions on how other kernels of certain attributes might be grouped.

**R stuff**

There are a few key libraries to install and load for our analysis. To perform Linear Discriminant Analysis in R we will make use of the `lda` function
in the package `MASS`. We will also use `car` and `ggplot2` for graphing purposes.

```{r libraries, echo=TRUE}
library(car)
library(MASS)
library(ggplot2)
```

### Description of Data

After reading in our dataset and grouping our catagorical variable, we can view the summaries of our variables of interest using the `summary` function.



```{r summary, echo=TRUE}
seeds = read.csv("./seeds.csv", sep="\t", header=TRUE)
seeds$group = as.factor(seeds$group)
levels(seeds$group) = c("Kama", "Rosa", "Canadian")
summary(seeds)
scatterplotMatrix(seeds[1:7])
```

The purpose of linear discriminant analysis (LDA) in this example is to find the linear combinations of the original variables (the 7 geometric properties) that gives the best possible separation between the groups (wheat varieties) in our data set.

If we want to separate the wines by cultivar, the wines come from three different cultivars, so the number of groups is $G=3$, and the number of variables is $p=7$. The maximum number of useful discriminant functions that can separate the wheat types by geometric properties is the minimum of $G-1$ and $p$, and so in this case it is the minimum of 2 and 7, which is 2. Thus, we can find at most 2 useful discriminant functions to classify our wheat types.

### Train/Test Sequence

In order to use LDA, we need to first split the data into a part used to train the classifier, and another part that will be used to test the classifier. For this example we will try an 70:30 split

```{r set_train, echo=TRUE}
set.seed(123) 
seedss = sample.int(n = nrow(seeds), size = floor(.7*nrow(seeds)), replace = F)
train = seeds[seedss, ]
test  = seeds[-seedss, ]
```

We are then able to train our classifier in the following way:

```{r train, echo=TRUE}
lseeds = lda(group~., train)
lseeds
```

This means that the first discriminant function is a linear combination of the variables: $$0.166*Area+3.299*Perimeter...+3.144*Groove$$.

The "proportion of trace" that is printed when you type "lseeds" (the variable returned by the `lda` function) is the proportion of between-class variance that is explained by successive discriminant functions.

Now with our train data model, we can predict our classifications with our test data:

```{r test, echo=TRUE}
lseeds.values = predict(lseeds, test[,1:7])
```

This will make predictions on where each of these observations should be grouped, based only on their qualitative attributes.

```{r plot, echo=TRUE}
plot.data = data.frame(LD1=lseeds.values$x[,1], LD2=lseeds.values$x[,2], WheatType=test$group)
head(plot.data)                       
p <- ggplot(data=plot.data, aes(x=LD1, y=LD2)) +
  geom_point(aes(color=WheatType)) +
  theme_bw()
p
```

We can see that the wheat types are well separated in the scatterplot. The first discriminant function (x-axis) separates Rosa from the other types very well, but does not perfectly separate Kama and Canadian.

The second discriminant function (y-axis) achieves a good separation of Kama and Canadian, and a fair separation of Rosa and Kama, although it is not totally perfect, and Rosa and Canadian aren't well separated either.

To achieve a very good separation of the three types, it would be best to use both the first and second discriminant functions together, as we can see using these two dimensions separates the classifications reasonably well.

**STATA Stuff**

```{r, echo=FALSE, message=FALSE}
library(Statamarkdown)
stataexe <- "C:/VApps/Stata_SE/15/StataSE-64.exe"
#knitr::opts_chunk$set(engine = 'stata', engine.path=stataexe, comment = '')
```
  
### Description of Data

Now, we perform our analysis using STATA. First, we read in the Seeds data set and obtain some summary statistics to get an intial understanding of the data with which we will be working: 
```{stata first-Stata, echo=TRUE,  engine.path=stataexe, collectcode = T}
// import the seeds data from local directory, relabel, and summarize
import delimited seeds.csv
summarize area peri comp l w asym lgroove
```
  
Next, we create a matrix of graphs or correlations between the potential explanatory variables to gain an initial understanding of how they are related:  
  
```{stata second-Stata, engine.path=stataexe, echo=2, results = "hide"}
// graph correlations between the variables of interest
graph matrix area peri comp l w asym lgroove
graph export graph1.png, replace
```
  
```{r, echo = F}
knitr::include_graphics("graph1.png")
```
  
We see some strong correlations in the predictors, as there are several pairs of explanatory variables that have a reasonbly strong, approximately linear correlation.  
We should note that the purpose of the linear disciminant analysis (LDA) in this example is primarily classification. We are hoping to find linear combinations of the 7 potential predictors in the data set that yield the best possible separation among seeds based on their wheat variety. In other words, we want the best separation such that each group of seeds can be identified based on their other characteristics.

Note that we want to separate the seeds by their wheat variety. There are three possible wheat varieties, so the number of groups here is *G* = 3, and the number of possible predictors is *p* = 7. We have that the maximum number of useful discriminant functions that can separate the seeds by wheat variety is the minimum of *G - 1* and *p*, which in this cases would be *G - 1* or 2. Hence, we can find at most two useful discriminant functions to classify the seeds by wheat variety.

### Train/Test Sequence

To perform an LDA analysis, it is necessary to split the data into a training set and a test set such that the performance of the LDA functions can be measured. To do this, we select random entries from the data set such that 70% of the data can be used for training and the remaining 30% can be used for testing:  

```{stata third-Stata, engine.path=stataexe, echo=TRUE, results = "hide"}
// separate into training and test set
set seed 123
generate random = runiform()
sort random
generate trainsamp = _n <= 147

// save test set for later
preserve
keep if !trainsamp
save test.dta, replace
restore

// keep training 
keep if trainsamp
```

We then use the training set to perform our LDA analysis:  
  
```{stata fourth-Stata, engine.path=stataexe, echo=TRUE}
// perform LDA analysis
candisc area peri comp l w asym lgroove, group(group)
```
  
For our example, there are two discriminant dimensions. The F-ratio tests in the table labeled "Canonical linear discrimnant analysis" are both significant, which indicates that both dimensions are needed to describe the differences between the seeds.  

The next table, labeled "Standardized canonical discriminant function coefficients" gives us our two discriminant functions, estimated from the training data, which would then be:
$$-0.494*Area+1.997*Perimeter...+.721*Groove$$
and:
$$4.888*Area+-4.471*Perimeter...+1.598*Groove$$
  
The remaining output yields means and classification percentages (i.e. the proportion classified correctly) for the training data.  

Now, to visualize the separation, we have:

```{stata fifth-Stata, engine.path=stataexe, echo=-c(1,7), results = "hide"}
quietly candisc area peri comp l w asym lgroove, group(group)
// change labels for future plotting
label define lab2 1 K 2 R 3 C 
label values group lab2
// generate plot to show the separation
scoreplot, msymbol(i)
graph export graph2.png, replace
```

```{r, echo = F}
knitr::include_graphics("graph2.png")
```
  
We see that the first discriminant function appears to separate Rosa seeds from the other types well, though it does not seems to separate Canadian and Kama seeds very much.  
The second discriminant funciton appears to separate the Kama seeds fairly well from the others, but Rosa and Canadian seeds are not well-seperated. The plot suggests that both discriminant functions are necessary here to differentiate between the seed types.  

Finally, we apply our estimated functions from the training data to the test data to get an idea of the prediction error that comes from this LDA model:  

```{stata sixth-Stata, echo = -1, engine.path=stataexe, echo=TRUE}
quietly candisc area peri comp l w asym lgroove, group(group)
// calculate prediction error by finding incorrect predictions for test set
use test.dta, clear
predict outcome, classification
label values outcome labseed
count if outcome != group
```
  
We see that of the 63 observations in the test set, only 3 were incorrectly predicted by our trained LDA model. As such, the prediction error here is only 4.76%. Hence, the LDA approach appears to work well for this data, which is supported not only by the low prediction error, but also by the plot, which shows fairly clear separation for the seed varieties.





